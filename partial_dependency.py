from __future__ import print_function
import sklearn
import pandas as pd
import numpy as np

#from sklearn.ensemble.partial_dependence import plot_partial_dependence
#from sklearn.ensemble.partial_dependence import partial_dependence


from scipy.stats.mstats import mquantiles
from sklearn.utils.extmath import cartesian

def _grid_from_X(X, percentiles=(0.05, 0.95), grid_resolution=100):
    """Generate a grid of points based on the ``percentiles of ``X``.
    The grid is generated by placing ``grid_resolution`` equally
    spaced points between the ``percentiles`` of each column
    of ``X``.
    Parameters
    ----------
    X : ndarray
        The data
    percentiles : tuple of floats
        The percentiles which are used to construct the extreme
        values of the grid axes.
    grid_resolution : int
        The number of equally spaced points that are placed
        on the grid.
    Returns
    -------
    grid : ndarray
        All data points on the grid; ``grid.shape[1] == X.shape[1]``
        and ``grid.shape[0] == grid_resolution * X.shape[1]``.
    axes : seq of ndarray
        The axes with which the grid has been created.
    """
    if len(percentiles) != 2:
        raise ValueError('percentile must be tuple of len 2')
    if not all(0. <= x <= 1. for x in percentiles):
        raise ValueError('percentile values must be in [0, 1]')

    axes = []
    emp_percentiles = mquantiles(X, prob=percentiles, axis=0)
    for col in range(X.shape[1]):
        uniques = np.unique(X[:, col])
        if uniques.shape[0] < grid_resolution:
            # feature has low resolution use unique vals
            axis = uniques
        else:
            # create axis based on percentiles and grid resolution
            axis = np.linspace(emp_percentiles[0, col],
                               emp_percentiles[1, col],
                               num=grid_resolution, endpoint=True)
        axes.append(axis)

    return cartesian(axes), axes

def _predict(est, X_eval, output=None):
    """Calculate part of the partial dependence of ``target_variables``.
    The function will be calculated by calling the ``predict_proba`` method of
    ``est`` for classification or ``predict`` for regression on ``X`` for every
    point in the grid.
    Parameters
    ----------
    est : BaseEstimator
        A fitted classification or regression model.
    X_eval : array-like, shape=(n_samples, n_features)
        The data on which the partial dependence of ``est`` should be
        predicted.
    output : int, optional (default=None)
        The output index to use for multi-output estimators.
    Returns
    -------
    out : array, shape=(n_classes, n_points)
        The partial dependence function evaluated on the ``grid``.
        For regression and binary classification ``n_classes==1``.
    """
    if est._estimator_type == 'regressor':
        try:
            out = est.predict(X_eval)
        except NotFittedError:
            raise ValueError('Call %s.fit before partial_dependence' %
                             est.__class__.__name__)
        if out.ndim != 1 and out.shape[1] == 1:
            # Column output
            out = out.ravel()
        if out.ndim != 1 and out.shape[1] != 1:
            # Multi-output
            if not 0 <= output < out.shape[1]:
                raise ValueError('Valid output must be specified for '
                                 'multi-output models.')
            out = out[:, output]
    elif est._estimator_type == 'classifier':
        try:
            out = est.predict_proba(X_eval)
        except NotFittedError:
            raise ValueError('Call %s.fit before partial_dependence' %
                             est.__class__.__name__)
        if isinstance(out, list):
            # Multi-output
            if not 0 <= output < len(out):
                raise ValueError('Valid output must be specified for '
                                 'multi-output models.')
            out = out[output]
        out = np.log(np.clip(out, 1e-16, 1))
        out = np.subtract(out, np.mean(out, 1)[:, np.newaxis])
    else:
        raise ValueError('est must be a fitted regressor or classifier '
                         'model.')
    return out


# https://github.com/trevorstephens/scikit-learn/blob/d81f2719520a450c9371bb10632dbc3f7c181922/sklearn/partial_dependence.py
def partial_dependence(est, target_variables, grid=None, X=None, output=None,
                       percentiles=(0.05, 0.95), grid_resolution=100,
                       method='exact'):
    """Partial dependence of ``target_variables``.
    Partial dependence plots show the dependence between the joint values
    of the ``target_variables`` and the function represented
    by the ``est``.
    Read more in the :ref:`User Guide <partial_dependence>`.
    Parameters
    ----------
    est : BaseEstimator
        A fitted classification or regression model.
    target_variables : array-like, dtype=int
        The target features for which the partial dependency should be
        computed (size should be smaller than 3 for visual renderings).
    grid : array-like, shape=(n_points, len(target_variables))
        The grid of ``target_variables`` values for which the
        partial dependency should be evaluated (either ``grid`` or ``X``
        must be specified).
    X : array-like, shape=(n_samples, n_features)
        The data on which ``est`` was trained. It is used to generate
        a ``grid`` for the ``target_variables``. The ``grid`` comprises
        ``grid_resolution`` equally spaced points between the two
        ``percentiles``.
    output : int, optional (default=None)
        The output index to use for multi-output estimators.
    percentiles : (low, high), default=(0.05, 0.95)
        The lower and upper percentile used to create the extreme values
        for the ``grid``. Only if ``X`` is not None.
    grid_resolution : int, default=100
        The number of equally spaced points on the ``grid``.
    method : {'recursion', 'exact', 'estimated', 'auto'}, default='auto'
        The method to use to calculate the partial dependence function:
        - If 'recursion', the underlying trees of ``est`` will be recursed to
          calculate the function. Only supported for BaseGradientBoosting and
          ForestRegressor.
        - If 'exact', the function will be calculated by calling the
          ``predict_proba`` method of ``est`` for classification or ``predict``
          for regression on ``X``for every point in the grid. To speed up this
          method, you can use a subset of ``X`` or a more coarse grid.
        - If 'estimated', the function will be calculated by calling the
          ``predict_proba`` method of ``est`` for classification or ``predict``
          for regression on the median of ``X``.
        - If 'auto', then 'recursion' will be used if ``est`` is
          BaseGradientBoosting or ForestRegressor, and 'exact' used for other
          estimators.
    Returns
    -------
    pdp : array, shape=(n_classes, n_points)
        The partial dependence function evaluated on the ``grid``.
        For regression and binary classification ``n_classes==1``.
    axes : seq of ndarray or None
        The axes with which the grid has been created or None if
        the grid has been given.
    Examples
    --------
    >>> samples = [[0, 0, 2], [1, 0, 0]]
    >>> labels = [0, 1]
    >>> from sklearn.ensemble import GradientBoostingClassifier
    >>> gb = GradientBoostingClassifier(random_state=0).fit(samples, labels)
    >>> kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2)
    >>> partial_dependence(gb, [0], **kwargs) # doctest: +SKIP
    (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
    """
    # TODO: The pattern below required to avoid a namespace collision.
    # TODO: Move below imports to module level import at 0.22 release.
    #from .ensemble._gradient_boosting import _partial_dependence_tree
    #from .ensemble.gradient_boosting import BaseGradientBoosting
    #from .ensemble.forest import ForestRegressor

    if method == 'auto':
        if isinstance(est, (BaseGradientBoosting, ForestRegressor)):
            method = 'recursion'
        else:
            method = 'exact'
    if X is None:
        raise ValueError('X is required for method="exact" or "estimated".')
    else:
        n_features = X.shape[1]
    if (grid is None and X is None) or (grid is not None and X is not None):
        raise ValueError('Either grid or X must be specified')

    target_variables = np.asarray(target_variables, dtype=np.int32,
                                  order='C').ravel()

    if any([not (0 <= fx < n_features) for fx in target_variables]):
        raise ValueError('target_variables must be in [0, %d]'
                         % (n_features - 1))

    if X is not None:
        grid, axes = _grid_from_X(X[:, target_variables], percentiles,
                                  grid_resolution)
    else:
        assert grid is not None
        # don't return axes if grid is given
        axes = None
        # grid must be 2d
        if grid.ndim == 1:
            grid = grid[:, np.newaxis]
        if grid.ndim != 2:
            raise ValueError('grid must be 2d but is %dd' % grid.ndim)

    grid = np.asarray(grid, order='C')
    assert grid.shape[1] == target_variables.shape[0]

    if method == 'recursion':
        if isinstance(est, BaseGradientBoosting):
            n_trees_per_stage = est.estimators_.shape[1]
            n_estimators = est.estimators_.shape[0]
            learning_rate = est.learning_rate
        else:
            n_trees_per_stage = 1
            n_estimators = len(est.estimators_)
            learning_rate = 1.
        pdp = np.zeros((n_trees_per_stage, grid.shape[0],), dtype=np.float64,
                       order='C')
        for stage in range(n_estimators):
            for k in range(n_trees_per_stage):
                if isinstance(est, BaseGradientBoosting):
                    tree = est.estimators_[stage, k].tree_
                else:
                    tree = est.estimators_[stage].tree_
                _partial_dependence_tree(tree, grid, target_variables,
                                         learning_rate, pdp[k])
        if isinstance(est, ForestRegressor):
            pdp /= n_estimators
    elif method == 'exact':
        n_samples = X.shape[0]
        pdp = []
        for row in range(grid.shape[0]):
            X_eval = X.copy()
            for i, variable in enumerate(target_variables):
                X_eval[:, variable] = np.repeat(grid[row, i], n_samples)
            pdp_row = _predict(est, X_eval, output=output)
            if est._estimator_type == 'regressor':
                pdp.append(np.mean(pdp_row))
            else:
                pdp.append(np.mean(pdp_row, 0))
        pdp = np.array(pdp).transpose()
        if pdp.shape[0] == 2:
            # Binary classification
            pdp = pdp[1, :][np.newaxis]
        elif len(pdp.shape) == 1:
            # Regression
            pdp = pdp[np.newaxis]
    elif method == 'estimated':
        n_samples = grid.shape[0]
        X_eval = np.tile(np.median(X, 0), [n_samples, 1])
        for i, variable in enumerate(target_variables):
            X_eval[:, variable] = grid[:, i]
        pdp = _predict(est, X_eval, output=output)
        if est._estimator_type == 'classifier' and pdp.shape[1] == 2:
            # Binary classification
            pdp = pdp[:, 1][np.newaxis]
        if est._estimator_type == 'regressor':
            pdp = pdp[np.newaxis]
    else:
        raise ValueError('method "%s" is invalid. Use "recursion", "exact", '
                         '"estimated", or None.' % method)
    return pdp, axes